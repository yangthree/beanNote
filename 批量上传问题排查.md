# 批量上传问题排查

## 错误：InternalError, 服务内部错误

### 可能原因：

1. **数据量太大** - JSON 文件有 9551 行，一次性传入可能导致超时
2. **云函数执行超时** - 云函数默认超时时间有限制
3. **内存不足** - 一次性处理太多数据

### 解决方案：

#### 方案一：分批上传（推荐）⭐

**第一次上传（前 10 条测试）：**
```json
{
  "records": [只粘贴前 10 条记录],
  "batchSize": 10,
  "startIndex": 0
}
```

**如果成功，继续上传剩余数据：**
```json
{
  "records": [粘贴全部 JSON 数组内容],
  "batchSize": 10,
  "startIndex": 0
}
```

执行后，如果返回 `hasMore: true`，继续执行：
```json
{
  "records": [同样的全部内容],
  "batchSize": 10,
  "startIndex": 10
}
```

重复执行，每次将 `startIndex` 更新为上次返回的 `nextIndex`。

#### 方案二：减少 batchSize

将 `batchSize` 改为更小的值，比如 5：
```json
{
  "records": [全部内容],
  "batchSize": 5,
  "startIndex": 0
}
```

#### 方案三：分段上传 JSON

将 JSON 文件分成多个小文件（每个 100-200 条），分别上传。

### 检查步骤：

1. **先测试少量数据**
   - 只上传前 5-10 条记录
   - 确认云函数正常工作

2. **查看云函数日志**
   - 在云开发控制台 → **"云函数"** → **"日志"**
   - 查看详细的错误信息

3. **检查数据格式**
   - 确保 JSON 格式正确
   - 确保每条记录都有必要的字段

### 优化后的云函数特点：

- ✅ 支持分批处理（通过 `startIndex` 参数）
- ✅ 默认每批只处理 20 条（可调整）
- ✅ 详细的进度日志
- ✅ 更好的错误处理
- ✅ 返回下次继续的索引

